---
title: "p8105_hw3_xl3141"
author: "Xinyuan Liu"
date: "10/15/2021"
output: github_document
---
## Set options
```{r}
library(tidyverse)
library(p8105.datasets)
library(httr)
library(jsonlite)
library(ggridges)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

```{r}
data("instacart")
ncol(instacart)
nrow(instacart)
instacart %>% 
  group_by(product_name) %>% 
  summarise(n_obs = n()) %>% 
  filter(n_obs == max(n_obs)) ##choose the most popular product

day_prior = instacart %>% 
  group_by(user_id) %>% 
  select(user_id, days_since_prior_order) %>% 
  unique() ##get the days since prior order for each unique user

mean(pull(day_prior, days_since_prior_order))
  
```

The data has 15 columns and 1384617 rows in total. The variables of interest are *days since prior order*, *product name*, and *reordered*. The product that is mostly bought is banana which is bought 18726 times in total. The average days since prior order is 17 days.

```{r}
instacart %>% 
  group_by(aisle) %>% 
  summarise(n_obs = n()) %>% ##count the number of aisles
  mutate(rank = rank(desc(n_obs))) %>% 
  filter(rank < 2) ##rank the numbers and find the largest one
```

We have 134 aisles and most items are ordered from aisle "fresh vegetables".

```{r}
instacart %>% 
  group_by(aisle, department) %>% 
  summarize(n_obs = n()) %>% 
  filter(n_obs > 10000) %>% ##get aisles with number > 10000
  relocate(department) %>% 
  ggplot(aes(x = aisle, y = n_obs, color = department)) +
  geom_point(alpha = .3, size = 3) +
  labs(
    title = "number of items purchased in each aisles",
    x = "aisle name",
    y = "number of times purchased"
  ) +
  theme(axis.text.x = element_text(size = 6, angle = 30))

```

```{r}
instacart %>% 
  group_by(aisle, product_name) %>% 
  filter(aisle == "baking ingredients") %>% ##look for baking ingredients in aisle column
  summarize(n_obs = n()) %>% 
  mutate(rank = rank(desc(n_obs))) %>% ##rank based on popularity
  filter(rank < 4) %>% 
  knitr::kable()

instacart %>% 
  group_by(aisle, product_name) %>% 
  filter(aisle == "dog food care") %>% 
  summarize(n_obs = n()) %>% 
  mutate(rank = rank(desc(n_obs))) %>% 
  filter(rank < 4) %>% 
  knitr::kable()

 
instacart %>% 
  group_by(aisle, product_name) %>% 
  filter(aisle == "packaged vegetables fruits") %>% 
  summarize(n_obs = n()) %>% 
  mutate(rank = rank(desc(n_obs))) %>% 
  filter(rank < 4) %>%
  knitr::kable()
```

```{r}
library(lubridate)
instacart %>% 
  group_by(product_name) %>% 
  mutate(
    order_dow = order_dow + 1,
    order_dow = wday(order_dow, label = TRUE)) %>% ##make number data into word
  select(order_dow, order_hour_of_day) %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>% 
  summarize(mean = mean(order_hour_of_day)) %>% ##calculate the mean hour of the day based on group "product name" and "order dow"
  pivot_wider(
    names_from = "order_dow",
    values_from = "mean"
    ) %>% 
  knitr::kable() ## make it into a 2*7 table
```

## Problem 2

```{r data import}

var_names = 
  GET("https://chronicdata.cdc.gov/views/acme-vg9e.json") %>%
  content("text") %>%
  fromJSON() %>% 
  .[["columns"]] %>% 
  .[["name"]] %>% 
  .[1:22]

brfss_smart2010 = 
  GET("https://chronicdata.cdc.gov/views/acme-vg9e/rows.json") %>% 
  content("text") %>%
  fromJSON() %>% 
  .[["data"]]

row_as_tibble = function(row_as_list, var_names) {
  var_list = row_as_list[9:30]
  names(var_list) = var_names 
  var_list[sapply(var_list, is.null)] <- NULL
  as_tibble(var_list, validate = FALSE)
}

brfss_smart2010 = 
  brfss_smart2010 %>% 
  map(.x = ., ~row_as_tibble(.x, var_names)) %>% 
  bind_rows
```

```{r data cleaning}


brfss_smart2010 = 
  brfss_smart2010 %>% 
    janitor::clean_names() %>% 
    filter(topic == "Overall Health") %>% 
    mutate(response = factor(x = response, levels = c("Poor", "Fair", "Good", "Very Good", "Excellent"), ordered = TRUE)) %>% ##organize responses as a factortaking levels ordered from poor to excellent 
    drop_na(response)
  
```

```{r}
brfss_smart2010 %>% 
  filter(year == "2002") %>%
  group_by(locationabbr) %>% 
  summarize(n_obs = n()) %>% 
  filter(n_obs >= 7) %>% ## show states observed more than 7 in 2002
  knitr::kable()

brfss_smart2010 %>% 
  filter(year == "2010") %>%
  group_by(locationabbr) %>% 
  summarize(n_obs = n()) %>% 
  filter(n_obs >= 7) %>% ## show states observed more than 7 in 2010
  knitr::kable()
 
```

```{r PLOT}
excellent_df = 
  brfss_smart2010 %>% 
  filter(response == "Excellent") %>% ##include only excellent response
  group_by(locationabbr, year) %>% 
  summarize(mean_data_value = mean(as.integer(data_value))) ## take the mean of data value based on state and year

ggp_data_value = 
  excellent_df %>% 
  ggplot(aes(x = year, y = mean_data_value, color = locationabbr)) +
  geom_point() +
  geom_line(aes(group = locationabbr)) +
  labs(
    title = "mean data value across year for each state",
    x = "year",
    y = "mean data value"
  ) 

ggp_data_value

```

```{r}
brfss_smart2010 %>% 
  filter(locationabbr == "NY", year %in% c("2006", "2010")) %>% ## show only NY state data in 2006 and 2010
  group_by(response, year) %>% 
  summarise(mean_NY_data_value = mean(as.integer(data_value))) %>% ## take the mean data value based on response and year 
  ggplot(aes(response, mean_NY_data_value)) +
  geom_bar(stat = "identity") +
  facet_grid(. ~ year)
```

## Problem 3

```{r}
accel_df = 
  read_csv("accel_data.csv") %>% 
  janitor::clean_names()

accel_df = 
  accel_df %>% 
  mutate(
    total_activity = rowSums(accel_df[, 4:1443]),
    weekday_weekend = day,
    weekday_weekend = replace(weekday_weekend, weekday_weekend %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday"), "Weekday"),
    weekday_weekend = replace(weekday_weekend, weekday_weekend %in% c("Saturday", "Sunday"), "Weekend")
  )
```

The dataset describes the activity count on weekdays and weekendss over 5 weeks. The important variables include *week*, *day*, *weekday vs weekend*, and *total activity count across one day*. In total, we have 1440 activities observed everyday.

```{r}
accel_df %>% 
  select(week, day, total_activity) %>% 
  pivot_wider(
    names_from = "day",
    values_from = "total_activity"
  ) %>% ## make the total activity into a wider format based on day. 
  relocate(Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday) %>% 
  knitr::kable()
```

There is a significant decrease on the total activity count on Sunday for week 4 and 5

```{r}
accel_df %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "activity",
    values_to = "count",
    names_prefix = "activity_"
  ) %>% ## make the activity counts into a long format
  ggplot(aes(x = as.numeric(activity), y = count, color = day)) +
  geom_smooth(alpha = .1) 
```

In general, the patient has higher activity count during daytime compared to at night. The activity count spikes during the morning on Sunday and during the night time on Friday, which suggest that the patient usually have greater physical activity during those two time periods.